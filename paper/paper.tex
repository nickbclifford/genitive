%
% File acl2019.tex
%
%% Based on the style files for ACL 2018, NAACL 2018/19, which were
%% Based on the style files for ACL-2015, with some improvements
%%  taken from the NAACL-2016 style
%% Based on the style files for ACL-2014, which were, in turn,
%% based on ACL-2013, ACL-2012, ACL-2011, ACL-2010, ACL-IJCNLP-2009,
%% EACL-2009, IJCNLP-2008...
%% Based on the style files for EACL 2006 by 
%%e.agirre@ehu.es or Sergi.Balari@uab.es
%% and that of ACL 08 by Joakim Nivre and Noah Smith

\documentclass[11pt,a4paper]{article}
\usepackage[hyperref]{acl2019}
\usepackage[utf8]{inputenc}
\usepackage[russian,english]{babel}
\usepackage{times}
\usepackage{latexsym}
\usepackage{amssymb}
\usepackage{multirow}
\usepackage{tipa}

\usepackage{url}

\aclfinalcopy % Uncomment this line for the final submission
%\def\aclpaperid{***} %  Enter the acl Paper ID here

%\setlength\titlebox{5cm}
% You can expand the titlebox if you need extra space
% to show all the authors. Please do not make the titlebox
% smaller than 5cm (the original size); we will check this
% in the camera-ready version and ask you to change it back.

\newcommand\BibTeX{B\textsc{ib}\TeX}

\newcommand{\citenp}[1]{\citeauthor{#1} (\citeyear{#1})}

\title{Modeling the Learning of the Russian Genitive Case}

\author{George Vallejo \\
  \texttt{gavallejo02@uchicago.edu} \\\And
  Nick Clifford \\
  \texttt{nclifford@uchicago.edu} \\}

\date{}

\begin{document}
\maketitle
\begin{abstract}
  Several features of human language can be described through rules,
  and natively speaking children of languages are cognizant of these rules, 
  even without formal instruction. Prior research has attempted to model
  such learning in children using connectionist neural networks for
  paradigms such as the English past tense. We investigate how a gradient-descent
  neural network could acquire the rules for declining Russian nouns into
  the genitive case for both singular and plural nouns in a similar fashion. 
  We also test whether two different NNs for the singular and plural numbers
  can model the learning of these rules in children better than a single
  NN by comparing the models' performance to that of native Russian-speaking children.

\end{abstract}

\section{Credits}

  This paper is an adaptation of the paper
  \emph{On Learning the Past Tenses of English Verbs} by D. E. Rumelhart and
  J. L. McClelland.
  
  All code written for this paper, including X-SAMPA translation and model training,
  is available at \url{https://github.com/nickbclifford/genitive}.

\section{Introduction}

Many features of natural languages, such as verb conjugation, noun case
declension, and agglutination behave in ways that can be described
by sets of rules. Being cognizant of these rules is an important aspect
of language acquisition in children, because awareness of these rules allows
them to utilize newly acquired vocabulary to express new concepts in the same
manner they may have used in a previous utterance. For example, the most common
ending used to express the past tense in English is -ed, as in ``I waited at home''.
Awareness of this rule would allow a child to additionally express ``I hunted at
the park'' after learning the verb ``hunt'' using the same ending.

However, it is also often the case that these rules have exceptions or other
irregularities. In English, ablaut is a source of several irregular verbs'
past tense forms. For example, the past tense of ``run'' is ``ran'', and
a child aware of the basic past tense rule may incorrectly express
``I runned to the park'' as a result, a phenomenon known as overgeneralizing.
This phemoenon is also important for learning, as the corrections the child
receives improve their understanding of the past tense, ultimately allowing
them to generalize the past tense for more verbs with greater accuracy.

Prior research has attempted to create neural networks to function as
language acquisition devices (LAD) in order to model how children acquire
the English past tense. Most notably, \citenp{RMcC}
made use of a perceptron model to model the learning of English past
tense in three stages based on prior research into how children
acquire it, and obtained learning results that very closely
align with children's learning. We therefore investigate whether
the acquisition of the genitive case in Russian would yield similar results.

\subsection{Russian noun case morphology}

Grammatical cases are a system of marking nouns and noun modifiers in order
to indicate the noun (modifier)'s intended grammatical function in a sentence.
While historically English had grammatical cases, they survive in modern English
only through pronouns. Every pronoun in English has three different forms representing
three different cases. These are:

\begin{enumerate}
    \item Nominative: Marks the subject of a sentence: \emph{I} help him.
    \item Accusative: Marks the indirect object of a sentence: He helps \emph{me}.
    \item Genitive: Marks the possessor of a noun: \emph{My} book is great.
\end{enumerate}

Russian, being a member of the Slavic branch of the Indo-European languages,
has a fully functioning grammatical case system which is used for all nouns
unlike English. Nouns and noun modifiers in Russian inflect for case
by changing the ending of the word, and the exact manner by which an ending
changes depends on the ending, case, and number of the noun. Table \ref{declension} shows 
the different possible endings of the Russian nominative and genitive cases in 
the singular and plural. 

There are, of course, various irregularities in forming the genitive case
for certain nouns. Some nouns have suppletive roots, found only in the
plural form: for example, the noun \emph{\v{c}elovjek} (person) has the regular singular 
genitive form \emph{\v{c}elovjeka}. However, in the nominative plural, it becomes
\emph{ljudji}, and in the genitive plural \emph{ljudjej}, using an entirely different
noun stem for the plural cases.

The reason we chose the genitive case to study is because its formation,
much like the English past tense, is fairly regular in the singular; in a majority of cases,
the genitive form can be simply derived from the nominative by modifying the
noun's ending based on the table. We also chose it because the genitive plural,
in particular, tends to be more irregular. It is often the aspect of Russian
that is most difficult for learners of the language to comprehend, so
we wanted to investigate how a gradient-descent neural network would handle
acquiring the genitive case in singular and plural forms.

We also chose the genitive because it is a case that (with the exception of
indeclinable nouns, which we ignored for this paper) always changes the noun
in some way. This is contrasted with the accusative case, which can either
not change the noun, use the genitive form to represent the accusative, or 
use its own unique endings. Whichever scenario applies depends on not just 
the ending of the noun, but also its gender and animacy.

\begin{table}[t!]
\begin{center}
\begin{tabular}{|l|l|l}
\hline & \textbf{Singular} & \textbf{Plural} \\ \hline
\textbf{Nominative} & -a, -ja, -ija & -I\textbackslash, -i, -ii \\
\textbf{Genitive} & -I\textbackslash, -i, -ii & $\varnothing$, -', -ij \\ \hline
\textbf{Nominative} & $\varnothing$, -'/j, -ij & -I\textbackslash, -i, -ii, -je \\
\textbf{Genitive} & -a, -ja, -ija & -ov, -jej/jev, -ijev \\ \hline
\textbf{Nominative} & -o, -je & -a, -ja \\
\textbf{Genitive} & -a, -ja & $\varnothing$, -j, -jej \\ \hline
\textbf{Nominative} & -' & -i \\
\textbf{Genitive} & -i & -jej \\ \hline
\textbf{Nominative} & -mja & -mjena \\
\textbf{Genitive} & -mjeni & -mjon \\ \hline
\end{tabular}
\end{center}
\caption{\label{declension} Nominative and genitive endings for nouns with different endings, written with X-SAMPA. Note that the apostrophe ' is a palatalization character, corresponding to \foreignlanguage{russian}{ь} in Cyrillic. }
\end{table}

\section{Featural representation of Russian phonemes}

\citenp{RMcC} made use of a scheme to encode
English phoneme units, known as Wickelphones and Wickelfeatures, described
in \citenp{Wickelgren}. Wickelphones are phoneme units representing
each phone in a word as triples, which consist of the phoneme itself,
its predecessor to the left, and its successor on the right.
Wickelfeatures are a means of representing Wickelphones as distributed
patterns of activation by capturing one feature of the central phoneme,
one feature of the predecessor, and one feature of the successor.

The feature coding scheme we used to capture Russian phonemes did not
differ heavily from Rumelhart and McClelland's scheme. However, in order
to accomodate Russian's richer consonant inventory, we had to add an
extra dimension to the scheme to account for plain vs. palatalized
consonants, the latter of which are in abundance in Russian phonology.

In addition to accomodating Russian phonology for Wickelfeatures, we had to make some
decisions regards to what exact sounds are in Russian. Specifically,
the status of the close central unrounded vowel \textipa{/\textbari/} 
(written in Cyrillic as \foreignlanguage{russian}{ы}) as a separate 
vowel morpheme in Russian is a subject of heavy debate. Some linguists 
believe in a five-vowel analysis, meaning that the vowel \textipa{/\textbari/} 
is in complementary distribution with the close front unrounded vowel 
\textipa{/i/} (written in Cyrillic as \foreignlanguage{russian}{и}), 
such that the vowel phoneme \textipa{/i/} occurs after soft (i.e. 
palatalized) consonants, whereas the vowel phoneme \textipa{/\textbari/} 
occurs after hard (i.e. plain) consonants. Other linguists believe in a
six-vowel analysis, asserting \textipa{/\textbari/} as a separate phoneme.

Another set of phonemes with disputed status in Russian are palatalized
velar consonants (i.e. \textipa{/k\super j/}, \textipa{/g\super j/}, 
and \textipa{/x\super j/}) as well as the palatalized voiceless alveolar
sibilant affricate \textipa{/\t{ts}\super j/}. In most cases, the velar consonants
become soft when followed by front vowels (except in the case of a word
boundary between the consonant and vowel), and the affricate is generally
always hard. It is only in certain loanwords and foreign names that
the consonants may become palatalized in other contexts.

For the sake of our experiment, however, we have chosen to follow the
five-vowel analysis and not count the extra palatalized consonants
as Russian phonemes. The phonemic status of the vowel \textipa{/\textbari/}
is marginal, only occuring isolated in the verb \foreignlanguage{russian}{ыкать}
(to pronounce the sound \foreignlanguage{russian}{ы}) and in borrowed names
of places in Russia. By following the five-vowel analysis, we can account for
one less vowel, because \textipa{/\textbari/} and \textipa{/i/} are otherwise
in complementary distribution in all other environments. Likewise, because
the velar consonants are normally only palatalized following front vowels,
there is no need to account for such redundant information.

Table \ref{phonemes} shows the scheme that we used to categorize Russian phonemes
on five simple dimensions.

\begin{table*}[t!]
\centering
\begin{tabular}{cccccccccccccc}
    \multicolumn{2}{c}{} & \multicolumn{12}{c}{Place}\\\hline
    \multicolumn{2}{c}{} & \multicolumn{4}{c}{Front} & \multicolumn{4}{c}{Middle} & \multicolumn{4}{c}{Back}\\\hline
    \multicolumn{2}{c}{} & \multicolumn{2}{c}{Voiceless} & \multicolumn{2}{c}{Voiced} & \multicolumn{2}{c}{Voiceless} & \multicolumn{2}{c}{Voiced} & \multicolumn{2}{c}{Voiceless} & \multicolumn{2}{c}{Voiced}\\\hline
    \multicolumn{2}{c}{} & H & S & H & S & H & S & H & S & H & S & H & S\\\hline
    \multirow{2}{50px}{Interrupted} & Stop & p & p' & b & b' & t & t' & d & d' & k & - & g & -\\
    & Nasal & - & - & m & m' & - & - & n & n' & - & - & - & - \\
    \multirow{2}{50px}{Cont.Cons.} & Fricative & f & f' & v & v' & s & s' & z & z' & s\` & t\_s\textbackslash & z\` & -\\
                                        & Liq/Sem. & - & - & l & l' & t\_s & - & r & r' & x & - & - & j\\
    \multirow{2}{50px}{Vowel} & Close & - & - & i & - & - & - & - & - & - & - & u & -\\
                              & Open & - & - & e & - & - & - & a & - & - & - & o & -\\
\end{tabular}
\caption{\label{phonemes} Categorization of Russian phonemes on five dimensions.
All phonemes are written using X-SAMPA.}
\end{table*}

\section{Experimental setup}

To run our experiment, we first obtained a Russian word list, filtered the
word data, then preprocessed the data into a format that can be used by a
neural network model. We then trained our neural network models based on
our word data and evaluated the results. The procedure for these steps
is outlined in the subsections below.

\subsection{Collecting Russian nouns and declensions}

In order to train the neural network, we needed a list of Russian nouns
and their genitive singular and plural forms. For this, we scraped words
from Wiktionary. Wiktionary is an online, multilingual, and free project
to create a dictionary for all natural languages. These dictionary entries
often contain additional information about words in languages, such as verb
conjugations, example sentences, synonyms and antonyms, and most importantly,
noun declensions.

Every Russian noun with an entry on Wiktionary is accessible through the category
page \textbf{Category:Russian Nouns}. Using Wiktionary's MediaWiki API, we gathered
every noun in this category in batches of 500 (due to API limitations). For each
batch, we checked to make sure that the word only contained letters in the modern
Russian Cyrillic alphabet, because Wiktionary also has entries for Russian words
containing characters from different alphabets (e.g. \textbf{\foreignlanguage{russian}{QR-код}},
meaning ``QR code'') as well as alternate spellings of words containing archaic letters 
unused since the Russian spelling reform of 1918.

After performing this initial pruning, we then queried the API for each word's
entry in Wiktionary and attempted to extract its genitive singular and plural
forms from the declension table available in the entry. If either the singular
or plural genitive forms were missing from the table, the noun is discarded.
This can happen if a word is an indeclinable loanword or only exists in either
the singular or plural forms. We then save the word, its genitive singular and
plural forms into a wordlist, which will be consulted during the pre-processing
and training phases.

\subsection{Noun data pre-processing}

Before the noun data can be used as input to train the neural network, it must
first be converted into a representation usable by the model.

The first transformation that takes place is converting the noun forms'
Russian Cyrillic representation into a phonemic representation in X-SAMPA\@.
Because the Russian alphabet is much more phonemically consistent than English,
meaning it is possible to derive a word's phonemic representation by iterating
through its Cyrillic representation one letter at a time.

However, phonemic representations are not specific enough for the neural network.
As previously referenced, \citenp{RMcC} refer to \citenp{Wickelgren}
to devise a featurization system using Wickelphones and Wickelfeatures.
In particular, each Wickelphone has a set of at most 25 Wickelfeatures that its presence activates,
which are derived from the phonological features of all three phonemes present in the Wickelphone.

For all phonemes other than the special word-boundary marker (which has a single unique `boundary' feature),
each phoneme has five features that correspond to the dimensions outlined in Table \ref{phonemes}.
For each feature in the central phoneme of the Wickelphone, at most five Wickelfeatures are generated:
each feature in the left context phoneme, the selected feature of the central phoneme,
and each feature in the right context. An example of this for the Wickelphone
\textbf{\textsubscript{j}a\textsubscript{z}} is present in Table \ref{features}.
Computationally, this algorithm corresponds well to a series of vector repetitions and a matrix transposition.

Each of these activated Wickelfeatures are then ``blurred": for a Wickelfeature $\left< f_1, f_2, f_3 \right>$,
all Wickelfeatures of the form $\left< * , f_2, f_3 \right>$ and $\left< f_1, f_2, * \right>$ have a random chance of being activated
as well. \citenp{RMcC} claim that this improves the neural network's ability to generalize between
phonological features.

However, to avoid computational blowup, Wickelfeatures with different dimensions for their left and right features
(other than the special word boundary feature) are filtered out of the computation. This brings the total number
of possible Wickelfeatures down from 2028 to only 660.

Activations across Wickelfeatures are thus represented by a boolean 660-vector, where a 1 in index $k$ corresponds to
Wickelfeature $k$ being activated. A word's total activation is all the Wickelfeatures activated by the word's composite Wickelphones. This is easily implemented with a vector sum operation.

\begin{table}[t!]
\begin{center}
	\small
\begin{tabular}{r|c|c|c}
	\# & Left Context & Central Phoneme & Right Context \\ \hline
	1 & Continuous & Vowel & Continuous \\
	2 & Liquid & Vowel & Fricative \\
	3 & Back & Vowel & Middle \\
	4 & Voiced & Vowel & Voiced \\
	5 & Soft & Vowel & Hard \\
	6 & Continuous & Open & Continuous \\
	7 & Liquid & Open & Fricative \\
	8 & Back & Open & Middle \\
	9 & Voiced & Open & Voiced \\
	10 & Soft & Open & Hard \\
	11 & Continuous & Middle & Continuous \\
	12 & Liquid & Middle & Fricative \\
	13 & Back & Middle & Middle \\
	14 & Voiced & Middle & Voiced \\
	15 & Soft & Middle & Hard \\
	16 & Continuous & Voiced & Continuous \\
	17 & Liquid & Voiced & Fricative \\
	18 & Back & Voiced & Middle \\
	19 & Voiced & Voiced & Voiced \\
	20 & Soft & Voiced & Hard \\
	21 & Continuous & Hard & Continuous \\
	22 & Liquid & Hard & Fricative \\
	23 & Back & Hard & Middle \\
	24 & Voiced & Hard & Voiced \\
	25 & Soft & Hard & Hard \\
\end{tabular}
\end{center}
\caption{\label{features} The Wickelfeatures activated by the Wickelphone
		                  \textbf{\textsubscript{j}a\textsubscript{z}}.}
\end{table}

\subsection{Neural network}

The basis of our neural network is a gradient-descent neural network powered by the
\emph{PyTorch} library. It uses a single linear transformation layer with the
input size being the number of Wickelfeatures that can be activated for,
and the output size being either 1 or 2 times the number of Wickelfeatures
that can be activated for, depending on whether the model is one of the
two smaller models learning individual genitive cases or the larger model
learning both genitive cases.

During training, the network's input is the activation vector for the nominative singular form
of a noun, and the expected output is the activation vector for that noun's genitive form
(singular or plural, depending on the network being trained). However, the larger model attempting
to learn both cases at once simply has the singular and plural genitive activation vectors concatenated together,
with the intent being that the model will eventually output two ``words" at once.

\subsection{Decoding model output}
Described in the Appendix of their paper, \citenp{RMcC} use what they term a ``binding network" to decode the feature activation vectors
outputted by the learning network into a full phonological representation. Because the activation
vector encoding scheme has no representation of the temporal dimension of a word (i.e., it does
not directly encode \textit{where} features and source phonemes occur in the source word), a separate
decoder is necessary. The binding network does not learn its weights, but rather each weight is determined
by the strength of its previous output ``in time", proportional to the probability of the corresponding feature
being activated (i.e., the network output).

The network attempts to model how each Wickelphone contributes to the Wickelfeatures present in the activation
vector, with the idea being ``to find a set of output features [Wickelphones] that accounts for as many as possible of the output features while minimizing the number of input features [Wickelfeatures] accounted for by more than one output feature. Thus, we want each of the output features to \textit{compete} for input features" \cite[pp. 269]{RMcC}.

\section{Results}
Unfortunately, our results were decidedly inconclusive.
The neural networks did successfully learn during training,
with a final loss of approximately 0.1 (down from 0.16 after 100 epochs).
However, the decoder network did not work as hoped, and only output two Wickelphones
when given output from any of the neural networks. For example, when given the network predicted activations
for the input \foreignlanguage{russian}{язык} (X-SAMPA \textbf{jazik}), the decoder only ever output
the Wickelphones \textbf{\textsubscript{\#}j\textsubscript{a}} and \textbf{\textsubscript{j}a\textsubscript{\#}};
for the input \foreignlanguage{russian}{жена} (X-SAMPA \textbf{z'ena}), the decoder only returned \textbf{\textsubscript{\#}z'\textsubscript{e}} and \textbf{\textsubscript{z'}e\textsubscript{\#}}.


\subsection{Discussion}
These results seem to suggest that a featural representation of words that properly preserves temporal
information is necessary to get meaningful output from these models. It is also possible that the
specification for the binding network in \citenp{RMcC} was overly general, and thus some nuance in implementation
may have been missed.

\section{Conclusion}

Although we were unable to adequately test our neural network hypotheses due to the
failure of our decoder network in converting the networks' outputs back to a phonological
representation, we have nonetheless shown that the networks are capable of learning the
genitive case's patterns to some extent with the Wickelphone/Wickelfeature schema described
in this paper. Further investigation of the intended implementation of the decoder network
could potentially demonstrate definitively whether or not the neural networks truly acquired 
the proper formation of the genitive case, and which of the two sets of networks more closely
models the acquisition of these cases in children with Russian as a native language.

Future work in this subject matter could use our design to model learning with the other cases 
of Russian, namely the dative, instrumental, and prepositional cases. It could also explore the
acquisition of the different forms of adjectives, which also decline for grammatical case and
number, or potentially the outcome of trying to learn both forms at once. All of these cases
tend to show greater degrees of regularity compared to the genitive case, meaning that a neural network
learning one or more of these cases is more likely to yield better results. This design
may not suit modeling of learning the Russian accusative case, however, because the endings used in the accusative depends on whether the noun is animate (i.e. a living being) or not.

\bibliography{paper}{}
\bibliographystyle{acl_natbib}
\end{document}
